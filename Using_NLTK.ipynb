{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using_NLTK.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natasolo/NLP_course/blob/main/Using_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz2OdUTK9vL6",
        "outputId": "64030c4a-e5a6-4797-9dcd-ab82ca4675ed"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDA9lglhQbE9",
        "outputId": "66f1f4a8-74ea-485f-a9b8-0f88e0ef31d8"
      },
      "source": [
        "nltk.download('punkt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2BWuYQ291SE"
      },
      "source": [
        "sentence = \"John and Emily are from India \""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIOSGfr8933N",
        "outputId": "646dadda-187f-4172-d69a-2ef177fb8d4b"
      },
      "source": [
        "# Get tokens\n",
        "from nltk import word_tokenize\n",
        "tokens = word_tokenize(sentence)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['John', 'and', 'Emily', 'are', 'from', 'India']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fskgpyht-WpD",
        "outputId": "6fb7bf1c-4f43-429b-9a61-ade73904cc8f"
      },
      "source": [
        "# Get pos tags\n",
        "tokens=word_tokenize(sentence)\n",
        "tokens_pos=pos_tag(tokens)\n",
        "print(tokens_pos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('John', 'NNP'), ('and', 'CC'), ('Emily', 'NNP'), ('are', 'VBP'), ('from', 'IN'), ('India', 'NNP'), (',', ','), ('they', 'PRP'), ('work', 'VBP'), ('at', 'IN'), ('Yahoo', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QF_FJZ4U-wFo",
        "outputId": "c14e166c-390b-4594-e194-7c092270f31c"
      },
      "source": [
        "# Get stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "tokens=word_tokenize(sentence)\n",
        "porter = [porter.stem(token) for token in tokens]\n",
        "print(porter)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['john', 'and', 'emili', 'are', 'from', 'india', ',', 'they', 'work', 'at', 'yahoo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8a3_VSwBWGZ",
        "outputId": "65e7c225-eb4d-4e0b-e670-be9c7f1d635d"
      },
      "source": [
        "# Get synonyms/antonyms\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "synonyms = []\n",
        "antonyms = []\n",
        "for syn in wordnet.synsets(\"work\"):\n",
        "    for l in syn.lemmas():\n",
        "        synonyms.append(l.name())\n",
        "        if l.antonyms():\n",
        "            antonyms.append(l.antonyms()[0].name())\n",
        "  \n",
        "print(set(synonyms))\n",
        "print(set(antonyms))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "{'make_for', 'operate', 'mould', 'employment', 'function', 'form', 'shape', 'wreak', 'ferment', 'play', 'work_on', 'exercise', 'study', 'turn', 'go', 'act_upon', 'exploit', 'forge', 'lick', 'piece_of_work', 'workplace', 'figure_out', 'puzzle_out', 'do_work', 'mold', 'crop', 'body_of_work', 'run', 'process', 'knead', 'bring', 'make', 'cultivate', 'solve', 'work_out', 'influence', 'sour', 'act', 'oeuvre', 'put_to_work', 'work'}\n",
            "{'malfunction', 'idle'}\n"
          ]
        }
      ]
    }
  ]
}